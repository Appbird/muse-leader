<!DOCTYPE html>
<html lang="ja">

<head>
    <meta charset="UTF-8">
    <link rel="stylesheet" href="css/normalize.css">
    <link rel="stylesheet" href="css/skeleton.css">
    <link rel="stylesheet" href="css/utility.css">

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Kosugi+Maru&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=Noto+Sans+JP:wght@100..900&display=swap" rel="stylesheet">

    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>MuseLeader: Toward Music Editing through Time-series Semantic Parameters Control using Large Language Model</title>
</head>

<body>
    <header>
        <div class="container">
            <h1 class="u-pagetitle">MuseLeader: Toward Music Editing through Time-series Semantic Parameters Control
                using Large Language Model</h1>
            <ul>
                <li>anonymous (_________________)</li>
                <li>anonymous (_________________)</li>
            </ul>

        </div>
    </header>
    <div class="container">
        <div class="row">
            <section>
                <h2>Paper</h2>
                <li>under review; (double-blind peer review)</li>
            </section>
            <section>
                <h2>Abstract</h2>
                <p>
                    As an application of generative AI, systems that allow users to control the content of generated
                    outputs
                    through parameters are being actively developed.
                    For content with temporal structures, such as music and stories,
                    systems have been proposed that <span class="u-strong"> enable editing using adjectives as
                        time-series parameters.</span>
                    For example, in the story generation system TaleBrush, users can set the main character's happiness
                    level along a timeline,
                    while in the music generation system SOUNDRAW, users can adjust the intensity of the music every
                    four measures.
                    However, these systems have fixed controllable parameters.
                    To address this limitation, we propose a system that <span class="u-strong">leverages large language
                        models (LLMs) to enable users
                        to define custom parameters for music generation.</span>
                    As a result,
                    we confirmed that users can control generated outputs in a time-series manner based on specific
                    concept (e.g. "strength," "elation."), although the concept remains somewhat limited.
                    This paper reports on these findings and discusses future challenges.
                </p>
            </section>

            <section>
                <h2>Interface Design</h2>
                <div class="u-center">
                    <img src="img/fig1_interface_of_muse_leader.jpg">
                </div>
                <div class="u-center">
                    <p>Fig.1 Interface of MuseLeader</p>
                </div>
                Time-series semantic parameters provide an interface
                to capture the evolution of a musical piece over time
                by specifying a mood and its intensity transition.
                Users adjust the mood's intensity at different time points
                by dragging circular markers (Figure 1(b)),
                and they select a specific mood by entering its name in the accompanying text box (Figure 1(a)).
                For example, entering "intensity" prompts the system to manage changes in the music's intensity;
                throughout the text, this is referred to as "the parameter axis."
                When the submit button is pressed, the system generates a musical piece with both a melody and a chord progression,
                assigning instruments accordingly.
                After generation, modifying parameters updates only the corresponding portions while preserving the overall motif;
                if only a subset of parameters is changed, only those specific sections are updated.
                
                
            </section>

            <section>
                <h2>Editing Examples</h2>
                <p>
                    The mood's intensity is represented as a numerical value between <strong>0.0 and 1.0</strong> and can be set for <strong>every 4 measures</strong>.
                    The higher the intensity value in a given section, the more strongly that mood is expected to be reflected in the music.
                </p>
                
                <p>The music pieces were generated using <strong>gpt-4o-2024-11-20</strong>.</p>
                <table>
                    <tr>
                        <th>Parameter-Axis</th>
                        <th>(a) 0.00 → 0.25 → 0.50 → 1.00</th>
                        <th>(b) 0.00 → 0.25 → 1.00 → 1.00</th>
                        <th>(c) 1.00 → 0.75 → 0.50 → 0.00</th>
                    </tr>              
                    <tr>
                        <td>Strength</td>
                        <td><audio controls><source src="audio/strength/composition-0.mp3" type="audio/mpeg"></audio></td>
                        <td><audio controls><source src="audio/strength/composition-1.mp3" type="audio/mpeg"></audio></td>
                        <td><audio controls><source src="audio/strength/composition-2.mp3" type="audio/mpeg"></audio></td>
                    </tr>
                    <tr>
                        <td>Robotic</td>
                        <td><audio controls><source src="audio/robotic/composition-0.mp3" type="audio/mpeg"></audio></td>
                        <td><audio controls><source src="audio/robotic/composition-1.mp3" type="audio/mpeg"></audio></td>
                        <td><audio controls><source src="audio/robotic/composition-2.mp3" type="audio/mpeg"></audio></td>
                    </tr>
                    <tr>
                        <td>Brightness</td>
                        <td><audio controls><source src="audio/brightness/composition-0.mp3" type="audio/mpeg"></audio></td>
                        <td><audio controls><source src="audio/brightness/composition-1.mp3" type="audio/mpeg"></audio></td>
                        <td><audio controls><source src="audio/brightness/composition-2.mp3" type="audio/mpeg"></audio></td>
                    </tr>      
                    <tr>
                        <td>Classic</td>
                        <td><audio controls><source src="audio/classic/composition-0.mp3" type="audio/mpeg"></audio></td>
                        <td><audio controls><source src="audio/classic/composition-1.mp3" type="audio/mpeg"></audio></td>
                        <td><audio controls><source src="audio/classic/composition-2.mp3" type="audio/mpeg"></audio></td>
                    </tr>
                    <tr>
                        <td>Jazziness</td>
                        <td><audio controls><source src="audio/jazz/composition-0.mp3" type="audio/mpeg"></audio></td>
                        <td><audio controls><source src="audio/jazz/composition-1.mp3" type="audio/mpeg"></audio></td>
                        <td><audio controls><source src="audio/jazz/composition-2.mp3" type="audio/mpeg"></audio></td>
                    </tr>
                    <tr>
                        <td>Urban</td>
                        <td><audio controls><source src="audio/citiness/composition-0.mp3" type="audio/mpeg"></audio></td>
                        <td><audio controls><source src="audio/citiness/composition-1.mp3" type="audio/mpeg"></audio></td>
                        <td><audio controls><source src="audio/citiness/composition-2.mp3" type="audio/mpeg"></audio></td>
                    </tr>
                    <tr>
                        <td>Heart-Pounding</td>
                        <td><audio controls><source src="audio/heart-pounding/composition-0.mp3" type="audio/mpeg"></audio></td>
                        <td><audio controls><source src="audio/heart-pounding/composition-1.mp3" type="audio/mpeg"></audio></td>
                        <td><audio controls><source src="audio/heart-pounding/composition-2.mp3" type="audio/mpeg"></audio></td>
                    </tr>
                    <tr>
                        <td>Emotional</td>
                        <td><audio controls><source src="audio/emotional/composition-0.mp3" type="audio/mpeg"></audio></td>
                        <td><audio controls><source src="audio/emotional/composition-1.mp3" type="audio/mpeg"></audio></td>
                        <td><audio controls><source src="audio/emotional/composition-2.mp3" type="audio/mpeg"></audio></td>
                    </tr>
                </table>
                
                <h3> Tendency </h3>
                <ul>
                    <li> (+) Axis closely tied to a musical style and related to instrument selection can be handled.</li>
                    <li> (+) Only sections with changed parameters are subject to editing, preserving the motif of the piece.</li>
                    <li> (-) Concepts that can have multiple musical styles, abstract concepts, or those that complicate chord progressions cannot be handled.</li>
                    <li> (-) Variations in music editing are limited.</li>
                </ul>
                
            </section>
            <section>
                <h2>Implementation</h2>
                <div class="u-center">
                    <img src="img/fig2_system_composition_of_muse_leader.jpg">
                </div>
                <div class="u-center">
                    <p>Fig.2 System composition of MuseLeader</p>
                </div>
                <h3> on gpt4o-2024-08-06 </h3>
                <p>We utilize the inference capabilities of a large-scale language model (gpt-4o-2024-08-06) to associate specified words with editing operations along the axis designated by the user.
                However, the system struggles to accurately reflect user intent in time-series control.
                To address this issue, we refine the LLM's prompts.
                </p>
                <p><span class="u-strong">Task Decomposition:</span>
                        Inspired by ComposerX,
                        our system splits the composition task among multiple LLM agents—a leader, melody, chord, and instrument agent.
                        
                </p>
                
                <p><span class="u-strong">Planning of Musical Elements:</span>
                        The leader plan musical elements in four-measure segments
                        documented in a table. Other agents use it to decide which sections to edit.
                    </p>
                <p><span class="u-strong">Use of Four-Measure Delimiters:</span>
                    our system instructs the LLM to insert newlines every four measure and comments (e.g., <code>% measure [start]-[end]</code>)
                    to clearly show which sections should be edited.
                </p>
                
                <p>Below are examples of prompts actually used for each agent.</p>
                <ol>
                    <li>
                        <strong>Leader Agents:</strong>
                        <a href="./prompt/leader.txt"
                            target="_blank">
                            prompt example
                        </a>
                    </li>
                    <li>
                        <strong>Chord Agents:</strong>
                        <a href="./prompt/chord_agent.txt"
                            target="_blank">
                            prompt example
                        </a>
                    </li>
                    <li>
                        <strong>Instrumet Agent:</strong>
                        <a href="./prompt/instrument_agent.txt"
                            target="_blank">
                            prompt example
                        </a>
                    </li>
                    <li>
                        <strong>Melody Agent:</strong>
                        <a href="./prompt/melody_agent.txt"
                            target="_blank">
                            prompt example
                        </a>
                    </li>
                </ol>

                <p>
                    This paper does not investigate the optimality of these prompt engineering techniques.
                    Establishing reliable evaluation methods and optimizing prompt design are important challenges for future research.
                    Moreover, these techniques may become unnecessary with advancements in the inference capabilities of large-scale language models in the future.
                </p>
            </section>
            <section>
                <h2>Tools</h2>
                <p>We use these tools for rendering abc format.</p>
                <ul>
                    <li>abc2midi ver 4.93</li>
                    <li>fluidsynth ver 2.3.4</li>
                    <li>soundfont: <a href="https://schristiancollins.com/generaluser.php">GeneralUser GS 1.471</a>
                    </li>
                </ul>
            </section>
        </div>
    </div>
</body>

</html>