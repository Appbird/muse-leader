<!DOCTYPE html>
<html lang="ja">

<head>
    <meta charset="UTF-8">
    <link rel="stylesheet" href="css/normalize.css">
    <link rel="stylesheet" href="css/skeleton.css">
    <link rel="stylesheet" href="css/utility.css">

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Kosugi+Maru&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=Noto+Sans+JP:wght@100..900&display=swap" rel="stylesheet">

    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>MuseLeader: Toward Music Editing through Time-series Semantic Parameters Control using Large Language Model</title>
</head>

<body>
    <header>
        <div class="container">
            <h1 class="u-pagetitle">MuseLeader: Toward Music Editing through Time-series Semantic Parameters Control
                using Large Language Model</h1>
            <ul>
                <li>Ryosei Kawaguchi (Kwansei Gakuin University)</li>
                <li>Katayose Haruhiro (Kwansei Gakuin University)</li>
            </ul>

        </div>
    </header>
    <div class="container">
        <div class="row">
            <section>
                <h2>Paper</h2>
                <li>Accepted @ <a href="https://cmmr2025.prism.cnrs.fr">CMMR 2025</a> / <a href="https://cmmr2025.prism.cnrs.fr/poster-oral/">Poster session #1 - 4 November 2025</a></li>
            </section>
            <section>
                <h2>Abstract</h2>
                <div class="u-center">
                    <iframe width="560" height="315" src="https://www.youtube.com/embed/GIP2G5y7_FY?si=mVSalGCIj7M-YehO" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>
                </div>
                
                <div class="u-center">
                    <img src="img/fig1_interface_of_muse_leader.svg">
                </div>
                <div class="u-center">
                    <p>Fig.1 Interface of MuseLeader</p>
                </div>
                
                <p>
                    Designing control methods in music generation systems is essential for music generation along with user preferences.
	In particular, parameter control provides an effective means of adjusting the atmosphere of generated music, such as ``brightness.''
	Additionally, some music generation systems allow users to specify transitions in atmospheric intensity.
	<span class="u-strong">However, parameter control is constrained by the frame problem</span>, where users can only manipulate the parameters
	predefined by the system. To overcome this limitation, this study proposes an
	approach that <span class="u-strong">leverages large language models (LLMs) to allow users to define parameter meanings through text.</span>
	We also introduce MuseLeader, a working music composition system. This is equipped
	with <span class="u-strong">a graphical user interface supporting customization of semantically defined time-series parameters</span>.
	User studies indicate that parameters with clear semantic definitions (e.g. ``Powerful,'' ``robotic'')
	can be effectively controlled according to user intent.
	Additionally, some users refine their expressive intentions through changing parameter axis.
	For further advancements, it is essential not only to enhance the inference capabilities of LLMs
	but also to explore multimodal inputs beyond text to improve the interpretation of complex
	and nuanced musical concepts.
                </p>
            </section>

            <section>
                <h2>Interface Design</h2>
                Time-series semantic parameters provide an interface
                to capture the evolution of a musical piece over time
                by specifying a mood and its intensity transition.
                Users adjust the mood's intensity at different time points
                by dragging circular markers (Figure 1(b)),
                and they select a specific mood by entering its name in the accompanying text box (Figure 1(a)).
                For example, entering "intensity" prompts the system to manage changes in the music's intensity;
                throughout the text, this is referred to as "the parameter axis."
                When the submit button is pressed, the system generates a musical piece with both a melody and a chord progression,
                assigning instruments accordingly.
                After generation, modifying parameters updates only the corresponding portions while preserving the overall motif;
                if only a subset of parameters is changed, only those specific sections are updated.
                
                
            </section>

            <section>
                <h2>Editing Examples</h2>
                <p>
                    The mood's intensity is represented as a numerical value between <strong>0.0 and 1.0</strong> and can be set for <strong>every 4 measures</strong>.
                    The higher the intensity value in a given section, the more strongly that mood is expected to be reflected in the music.
                </p>
                
                <p>The music pieces were generated using <strong>gpt-4o-2024-11-20</strong>.</p>
                <table>
                    <tr>
                        <th>Parameter-Axis</th>
                        <th>(a) 0.00 → 0.25 → 0.50 → 1.00</th>
                        <th>(b) 0.00 → 0.25 → 1.00 → 1.00</th>
                        <th>(c) 1.00 → 0.75 → 0.50 → 0.00</th>
                    </tr>              
                    <tr>
                        <td>Strength</td>
                        <td><audio controls><source src="audio/strength/composition-0.mp3" type="audio/mpeg"></audio></td>
                        <td><audio controls><source src="audio/strength/composition-1.mp3" type="audio/mpeg"></audio></td>
                        <td><audio controls><source src="audio/strength/composition-2.mp3" type="audio/mpeg"></audio></td>
                    </tr>
                    <tr>
                        <td>Robotic</td>
                        <td><audio controls><source src="audio/robotic/composition-0.mp3" type="audio/mpeg"></audio></td>
                        <td><audio controls><source src="audio/robotic/composition-1.mp3" type="audio/mpeg"></audio></td>
                        <td><audio controls><source src="audio/robotic/composition-2.mp3" type="audio/mpeg"></audio></td>
                    </tr>
                    <tr>
                        <td>Brightness</td>
                        <td><audio controls><source src="audio/brightness/composition-0.mp3" type="audio/mpeg"></audio></td>
                        <td><audio controls><source src="audio/brightness/composition-1.mp3" type="audio/mpeg"></audio></td>
                        <td><audio controls><source src="audio/brightness/composition-2.mp3" type="audio/mpeg"></audio></td>
                    </tr>      
                    <tr>
                        <td>Classic</td>
                        <td><audio controls><source src="audio/classic/composition-0.mp3" type="audio/mpeg"></audio></td>
                        <td><audio controls><source src="audio/classic/composition-1.mp3" type="audio/mpeg"></audio></td>
                        <td><audio controls><source src="audio/classic/composition-2.mp3" type="audio/mpeg"></audio></td>
                    </tr>
                    <tr>
                        <td>Jazziness</td>
                        <td><audio controls><source src="audio/jazz/composition-0.mp3" type="audio/mpeg"></audio></td>
                        <td><audio controls><source src="audio/jazz/composition-1.mp3" type="audio/mpeg"></audio></td>
                        <td><audio controls><source src="audio/jazz/composition-2.mp3" type="audio/mpeg"></audio></td>
                    </tr>
                    <tr>
                        <td>Urban</td>
                        <td><audio controls><source src="audio/citiness/composition-0.mp3" type="audio/mpeg"></audio></td>
                        <td><audio controls><source src="audio/citiness/composition-1.mp3" type="audio/mpeg"></audio></td>
                        <td><audio controls><source src="audio/citiness/composition-2.mp3" type="audio/mpeg"></audio></td>
                    </tr>
                    <tr>
                        <td>Heart-Pounding</td>
                        <td><audio controls><source src="audio/heart-pounding/composition-0.mp3" type="audio/mpeg"></audio></td>
                        <td><audio controls><source src="audio/heart-pounding/composition-1.mp3" type="audio/mpeg"></audio></td>
                        <td><audio controls><source src="audio/heart-pounding/composition-2.mp3" type="audio/mpeg"></audio></td>
                    </tr>
                    <tr>
                        <td>Emotional</td>
                        <td><audio controls><source src="audio/emotional/composition-0.mp3" type="audio/mpeg"></audio></td>
                        <td><audio controls><source src="audio/emotional/composition-1.mp3" type="audio/mpeg"></audio></td>
                        <td><audio controls><source src="audio/emotional/composition-2.mp3" type="audio/mpeg"></audio></td>
                    </tr>
                </table>
                
                <h3> Tendency </h3>
                <ul>
                    <li> (+) Axis closely tied to a musical style and related to instrument selection can be handled.</li>
                    <li> (+) Only sections with changed parameters are subject to editing, preserving the motif of the piece.</li>
                    <li> (-) Concepts that can have multiple musical styles, abstract concepts, or those that complicate chord progressions cannot be handled.</li>
                    <li> (-) Variations in music editing are limited.</li>
                </ul>
                
            </section>
            <section>
                <h2>Implementation</h2>
                <div class="u-center">
                    <img src="img/fig2_system_composition_of_muse_leader.svg">
                </div>
                <div class="u-center">
                    <p>Fig.2 System composition of MuseLeader</p>
                </div>
                <h3> on gpt4o-2024-08-06 </h3>
                <p>We utilize the inference capabilities of a large-scale language model (gpt-4o-2024-08-06) to associate specified words with editing operations along the axis designated by the user.
                However, the system struggles to accurately reflect user intent in time-series control.
                To address this issue, we refine the LLM's prompts.
                </p>
                <p><span class="u-strong">Task Decomposition:</span>
                        Inspired by ComposerX,
                        our system splits the composition task among multiple LLM agents—a leader, melody, chord, and instrument agent.
                        
                </p>
                
                <p><span class="u-strong">Planning of Musical Elements:</span>
                        The leader plan musical elements in four-measure segments
                        documented in a table. Other agents use it to decide which sections to edit.
                    </p>
                <p><span class="u-strong">Use of Four-Measure Delimiters:</span>
                    our system instructs the LLM to insert newlines every four measure and comments (e.g., <code>% measure [start]-[end]</code>)
                    to clearly show which sections should be edited.
                </p>

                <div class="u-center">
                    <img src="./img/fig3_multi_agent_LLM_based_composition.svg">
                </div>
                <div class="u-center">
                    <p>Fig.3 Workflow of Multi-Agent LLM-Based Music Composition and Arrangement</p>
                </div>

                <p>The following are actual prompts used for each agent, translated into English.</p>
                <ol>
                    <li>
                        <strong>Leader Agents:</strong>
                        <a href="./prompt_en/leader.txt"
                            target="_blank">
                            prompt example
                        </a>
                    </li>
                    <li>
                        <strong>Chord Agents:</strong>
                        <a href="./prompt_en/chord_agent.txt"
                            target="_blank">
                            prompt example
                        </a>
                    </li>
                    <li>
                        <strong>Instrumet Agent:</strong>
                        <a href="./prompt_en/instrument_agent.txt"
                            target="_blank">
                            prompt example
                        </a>
                    </li>
                    <li>
                        <strong>Melody Agent:</strong>
                        <a href="./prompt_en/melody_agent.txt"
                            target="_blank">
                            prompt example
                        </a>
                    </li>
                </ol>

                <p>
                    This paper does not investigate the optimality of these prompt engineering techniques.
                    Establishing reliable evaluation methods and optimizing prompt design are important challenges for future research.
                    Moreover, these techniques may become unnecessary with advancements in the inference capabilities of large-scale language models in the future.
                </p>
            </section>
            <section>
                <h2>Tools</h2>
                <p>We use these tools for rendering abc format.</p>
                <ul>
                    <li>abc2midi ver 4.93</li>
                    <li>fluidsynth ver 2.3.4</li>
                    <li>soundfont: <a href="https://schristiancollins.com/generaluser.php">GeneralUser GS 1.471</a>
                    </li>
                </ul>
            </section>
        </div>
    </div>
</body>

</html>