<!DOCTYPE html>
<html lang="ja">

<head>
    <meta charset="UTF-8">
    <link rel="stylesheet" href="css/normalize.css">
    <link rel="stylesheet" href="css/skeleton.css">
    <link rel="stylesheet" href="css/utility.css">

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Kosugi+Maru&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=Noto+Sans+JP:wght@100..900&display=swap" rel="stylesheet">

    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>MuseLeader</title>
</head>

<body>
    <header>
        <div class="container">
            <h1 class="u-pagetitle">MuseLeader: Toward Music Editing through Time-series Semantic Parameters Control
                using Large Language Model</h1>
            <ul>
                <li>anonymous (_________________)</li>
                <li>anonymous (_________________)</li>
            </ul>

        </div>
    </header>
    <div class="container">
        <div class="row">
            <section>
                <h2>Paper</h2>
                <li>under review; (double-blind peer review)</li>
            </section>
            <section>
                <h2>Abstract</h2>
                <p>
                    As an application of generative AI, systems that allow users to control the content of generated
                    outputs
                    through parameters are being actively developed.
                    For content with temporal structures, such as music and stories,
                    systems have been proposed that <span class="u-strong"> enable editing using adjectives as
                        time-series parameters.</span>
                    For example, in the story generation system TaleBrush, users can set the main character's happiness
                    level along a timeline,
                    while in the music generation system SOUNDRAW, users can adjust the intensity of the music every
                    four measures.
                    However, these systems have fixed controllable parameters.
                    To address this limitation, we propose a system that <span class="u-strong">leverages large language
                        models (LLMs) to enable users
                        to define custom parameters for music generation.</span>
                    As a result,
                    we confirmed that users can control generated outputs in a time-series manner based on specific
                    concept (e.g. "strength," "elation."), although the concept remains somewhat limited.
                    This paper reports on these findings and discusses future challenges.
                </p>
            </section>

            <section>
                <h2>Interface Design</h2>
                <div class="u-center">
                    <img src="img/fig1_interface_of_muse_leader.jpg">
                </div>
                <div class="u-center">
                    <p>Fig.1 Interface of MuseLeader</p>
                </div>
                Time-series semantic parameters provide an interface
                to capture the evolution of a musical piece over time
                by specifying a mood and its intensity transition.
                Users adjust the mood's intensity at different time points
                by dragging circular markers (Figure 1(b)),
                and they select a specific mood by entering its name in the accompanying text box (Figure 1(a)).
                For example, entering "intensity" prompts the system to manage changes in the music's intensity;
                throughout the text, this is referred to as "the parameter axis."
                When the submit button is pressed, the system generates a musical piece with both a melody and a chord progression,
                assigning instruments accordingly.
                After generation, modifying parameters updates only the corresponding portions while preserving the overall motif;
                if only a subset of parameters is changed, only those specific sections are updated.
                
                
            </section>

            <section>
                <h2>Editing Examples</h2>
                <p>This system presents the generated music pieces and the editing results through parameter control.</p>
    
                <p>
                    The mood's intensity is represented as a numerical value between <strong>0.0 and 1.0</strong> and can be set for every four measures.
                    The higher the intensity value in a given section, the more strongly that mood is expected to be reflected in the music.
                </p>
                
                <p>Example: An input of <strong>Brightness: 0.00 → 0.25 → 0.50 → 1.00</strong> means:</p>
                <ul>
                    <li>Measures 1–4 are expected to have no brightness.(0.00)</li>
                    <li>Measures 5–8 are expected to have little brightness. (0.25)</li>
                    <li>Measures 9–12 are expected to have a little brightness of 0.50</li>
                    <li>Measures 13–16 are expected to have brightness of 1.00</li>
                </ul>
                
                <h2>Generated Music Pieces</h2>
                <p>The music pieces listed below were generated based on the moods specified in the leftmost cell of each row (e.g., "Brightness") under the following input conditions:</p>
                <ul>
                    <li><strong>(a)</strong> The input <strong>0.00 → 0.25 → 0.50 → 1.00</strong> is provided.</li>
                    <li><strong>(b)</strong> Based on the generated piece (a), a slightly modified input <strong>0.00 → 0.25 → 0.50 → 1.00</strong> is given.</li>
                    <li><strong>(c)</strong> Based on the generated piece (b), a slightly modified input <strong>1.00 → 0.75 → 0.50 → 0.00</strong> is given.</li>
                </ul>
                
                <p>
                    For the moods used in generation, we adopt those that were used by participants in a user study.
                    The expressions in parentheses represent the original Japanese terms used by the participants, as some nuances are difficult to translate directly into English.
                </p>
                
                <p>The music pieces were generated using <strong>gpt-4o-2024-11-20</strong>.</p>
                <table>
                    <tr>
                        <th>Parameter-Axis</th>
                        <th>(a) 0.00 → 0.25 → 0.50 → 1.00</th>
                        <th>(b) 0.00 → 0.25 → 1.00 → 1.00</th>
                        <th>(c) 1.00 → 0.75 → 0.50 → 0.00</th>
                    </tr>
                    <tr>
                        <td>Brightness<br>(明るさ)</td>
                        <td><audio controls><source src="audio/brightness/composition-0.mp3" type="audio/mpeg"></audio></td>
                        <td><audio controls><source src="audio/brightness/composition-1.mp3" type="audio/mpeg"></audio></td>
                        <td><audio controls><source src="audio/brightness/composition-2.mp3" type="audio/mpeg"></audio></td>
                    </tr>                    
                    <tr>
                        <td>strength<br>(強さ)</td>
                        <td><audio controls><source src="audio/strength/composition-0.mp3" type="audio/mpeg"></audio></td>
                        <td><audio controls><source src="audio/strength/composition-1.mp3" type="audio/mpeg"></audio></td>
                        <td><audio controls><source src="audio/strength/composition-2.mp3" type="audio/mpeg"></audio></td>
                    </tr>
                    <tr>
                        <td>Robotic<br>(ロボット感)</td>
                        <td><audio controls><source src="audio/robotic/composition-0.mp3" type="audio/mpeg"></audio></td>
                        <td><audio controls><source src="audio/robotic/composition-1.mp3" type="audio/mpeg"></audio></td>
                        <td><audio controls><source src="audio/robotic/composition-2.mp3" type="audio/mpeg"></audio></td>
                    </tr>
                    <tr>
                        <td>Classic<br>(クラシック感)</td>
                        <td><audio controls><source src="audio/classic/composition-0.mp3" type="audio/mpeg"></audio></td>
                        <td><audio controls><source src="audio/classic/composition-1.mp3" type="audio/mpeg"></audio></td>
                        <td><audio controls><source src="audio/classic/composition-2.mp3" type="audio/mpeg"></audio></td>
                    </tr>
                    <tr>
                        <td>Jazziness<br>(ジャズ感)</td>
                        <td><audio controls><source src="audio/jazz/composition-0.mp3" type="audio/mpeg"></audio></td>
                        <td><audio controls><source src="audio/jazz/composition-1.mp3" type="audio/mpeg"></audio></td>
                        <td><audio controls><source src="audio/jazz/composition-2.mp3" type="audio/mpeg"></audio></td>
                    </tr>
                    <tr>
                        <td>Urban<br>(都市感)</td>
                        <td><audio controls><source src="audio/citiness/composition-0.mp3" type="audio/mpeg"></audio></td>
                        <td><audio controls><source src="audio/citiness/composition-1.mp3" type="audio/mpeg"></audio></td>
                        <td><audio controls><source src="audio/citiness/composition-2.mp3" type="audio/mpeg"></audio></td>
                    </tr>
                    <tr>
                        <td>Heart-Pounding<br>(トキメキ感)</td>
                        <td><audio controls><source src="audio/heart-pounding/composition-0.mp3" type="audio/mpeg"></audio></td>
                        <td><audio controls><source src="audio/heart-pounding/composition-1.mp3" type="audio/mpeg"></audio></td>
                        <td><audio controls><source src="audio/heart-pounding/composition-2.mp3" type="audio/mpeg"></audio></td>
                    </tr>
                    <tr>
                        <td>Emotional<br>(エモい)</td>
                        <td><audio controls><source src="audio/emotional/composition-0.mp3" type="audio/mpeg"></audio></td>
                        <td><audio controls><source src="audio/emotional/composition-1.mp3" type="audio/mpeg"></audio></td>
                        <td><audio controls><source src="audio/emotional/composition-2.mp3" type="audio/mpeg"></audio></td>
                    </tr>
                </table>
                
                <h3> Trends </h3>
                <ul>
                    <li> (+) Elements that are closely tied to a musical style and can be expressed through instrument selection can be handled.</li>
                    <li> (+) Only sections with changed parameters are subject to editing, preserving the motif of the piece.</li>
                    <li> (-) Concepts that can have multiple musical styles, abstract concepts, or those that complicate chord progressions cannot be handled.</li>
                    <li> (-) Variations in music editing are limited.</li>
                </ul>
                
            </section>
            <section>
                <h2>Implementation</h2>
                <div class="u-center">
                    <img src="img/fig2_system_composition_of_muse_leader.jpg">
                </div>
                <div class="u-center">
                    <p>Fig.2 System composition of MuseLeader</p>
                </div>
                <h3> on gpt4o-2024-08-06 </h3>
                <p>We utilize the inference capabilities of a large-scale language model (gpt-4o-2024-08-06) to associate specified words with editing operations along the axis designated by the user.
                However, the system struggles to accurately reflect user intent in time-series control.
                For example, when a user specifies a particular measure range and requests changes to the melody or chords,
                unintended sections or other musical elements may be significantly altered.
                This makes it difficult to reflects the user's intended progression.
                To address this issue, we refine the LLM's prompts.
                </p>
                <p><span class="u-strong">Task Decomposition:</span>
                        Inspired by ComposerX,
                        our system splits the composition task among multiple LLM agents—a leader, melody, chord, and instrument agent.
                        The leader designs the overall structure while the others sequentially determine chord progressions and melodies,
                        thereby reducing task complexity.
                </p>
                
                <p><span class="u-strong">Planning of Musical Elements:</span>
                        Following WavJourney,
                        which converts user instructions into a table of musical elements,
                        our system has the leader plan musical elements in four-measure segments
                        documented in a table. Other agents use it to decide which sections to edit.
                    </p>
                <p><span class="u-strong">Use of Four-Measure Delimiters:</span>
                    Drawing from MuPT, which employs a delimiter (<code><|></code>),
                    our system instructs the LLM to insert newlines every four measure and comments (e.g., <code>% measure [start]-[end]</code>)
                    to clearly show editable sections, simplifying the agents' identification of target areas.
                </p>
                
                <p>以下、実際に各エージェントに対して利用されたプロンプトを例示します。</p>
                <ol>
                    <li>
                        <strong>Leader Agents:</strong>
                        <a href="https://github.com/Appbird/muse-leader/blob/main/src/prompt/leader.txt"
                            target="_blank">
                            prompt example
                        </a>
                    </li>
                    <li>
                        <strong>Chord Agents:</strong>
                        <a href="https://github.com/Appbird/muse-leader/blob/main/src/prompt/chord_agent.txt"
                            target="_blank">
                            prompt example
                        </a>
                    </li>
                    <li>
                        <strong>Instrumet Agent:</strong>
                        <a href="https://github.com/Appbird/muse-leader/blob/main/src/prompt/instrument_agent.txt"
                            target="_blank">
                            prompt example
                        </a>
                    </li>
                    <li>
                        <strong>Melody Agent:</strong>
                        <a href="https://github.com/Appbird/muse-leader/blob/main/src/prompt/melody_agent.txt"
                            target="_blank">
                            prompt example
                        </a>
                    </li>
                </ol>

                <p>
                    These techniques have ensured the editing capability for time-series parameters.
                    However, this paper does not investigate the optimality of these prompt engineering techniques.
                    While the combined methods have been confirmed to be effective, it cannot be concluded that this particular prompt combination is the optimal approach for time-series control.
                    It is necessary to demonstrate the effectiveness of these techniques quantitatively and with a sufficient number of trials.
                    Establishing such evaluation methods and optimizing prompt design are important challenges for future research.
                    Moreover, these techniques may become unnecessary with advancements in the inference capabilities of large-scale language models in the future.
                </p>
            </section>
            <section>
                <h2>Tools</h2>
                <p>We use these tools for rendering abc format.</p>
                <ul>
                    <li>abc2midi ver 4.93</li>
                    <li>fluidsynth ver 2.3.4</li>
                    <li>soundfont: <a href="https://schristiancollins.com/generaluser.php">GeneralUser GS 1.471</a>
                    </li>
                </ul>
            </section>

            <section>
                <h2>Reference</h2>
                <ol>
                    <li></li>
                </ol>
            </section>
        </div>
    </div>
</body>

</html>